{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af3e1a5",
   "metadata": {},
   "source": [
    "## Key points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a0e57",
   "metadata": {},
   "source": [
    "1. Pernyataan masalah\n",
    "2. Memahami Dataset Churn Prediction\n",
    "3. Data Preprocessing Steps\n",
    "4. EDA ( Exploration Data Analysis )\n",
    "5. Model building\n",
    "6. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8532c",
   "metadata": {},
   "source": [
    "# 1. Pernyataan Masalah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf1e77",
   "metadata": {},
   "source": [
    "   Kita diberikan dataset nasabah bank, dan kita ingin membuat model prekdiksi churn yang dapat mengklarifikasikan nasabah menjadi dua kelas:\n",
    "   + Candidat Chrun \n",
    "   + Non Churn\n",
    "\n",
    "    \"Problem ini akan kita selesaikan dengan pendekatan supervise learning, artinya kita akan memberikan label  sample data input dan output\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c696f23b",
   "metadata": {},
   "source": [
    "# 2. Memahami dataset Churn prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb9384",
   "metadata": {},
   "source": [
    "Source dataset diambil dari ==> https://www.kaggle.com/datasets/mathchi/churn-for-bank-customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ba7b4",
   "metadata": {},
   "source": [
    "+ RowNumber — corresponds to the record (row) number and has no effect on the output.\n",
    "+ CustomerId — contains random values and has no effect on customer leaving the bank.\n",
    "+ Surname — the surname of a customer has no impact on their decision to leave the bank.\n",
    "+ CreditScore — can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n",
    "+ Geography — a customer’s location can affect their decision to leave the bank.\n",
    "+ Gender — it’s interesting to explore whether gender plays a role in a customer leaving the bank.\n",
    "+ Age — this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n",
    "+ Tenure — refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n",
    "\n",
    "    + Balance—also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n",
    "    + NumOfProducts—refers to the number of products that a customer has purchased through the bank.\n",
    "    + HasCrCard—denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n",
    "    + IsActiveMember—active customers are less likely to leave the bank.\n",
    "    + EstimatedSalary—as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n",
    "    + Exited—whether or not the customer left the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno # provides a small toolset of flexible and easy-to-use missing data visualizations \n",
    "                         # and utilities that allows you to get a quick visual summary of the completeness \n",
    "                         # (or lack thereof) of your dataset.\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,train_test_split,cross_val_score\n",
    "import itertools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import LocalOutlierFactor # çok değişkenli aykırı gözlem incelemesi\n",
    "from sklearn.preprocessing import scale,StandardScaler, MinMaxScaler,Normalizer,RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  accuracy_score, f1_score, precision_score,confusion_matrix, recall_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "import plotly.graph_objects as go #\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px  # digunakan untuk keperluan visualisasi data\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# to display all columns and rows:\n",
    "pd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);  # to display all columns and rows\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x) # The number of numbers that will be shown after the comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31378dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feb0db83",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "df = pd.read_csv('/Users/mac/Study/Study_burhanudin/Common_Data/churn.csv')\n",
    "\n",
    "\n",
    "# Kita amati info dataset kita\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62319ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita lihat sekilas data terseut\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4da84",
   "metadata": {},
   "source": [
    "+ Dapat di simpulkan bahwa :\n",
    "    + Rownumber, CustomerId dan Surename hanya mendeskripsikan pelanggan secara unik dan tidak menpengaruhi variable target, sehingga kita dapat menghapus kolom ini.\n",
    "    + Kita memiliki dua kolom category 'geografi' dan 'gender' yang akan kami encode menggunakan labelencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e0cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selanjutnya akan menampilkan nama-nama kolom pada dataframe\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f81ce",
   "metadata": {},
   "source": [
    "## Visualize missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan missing value dalam bentik matrix\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d907fc6e",
   "metadata": {},
   "source": [
    "+ Sekilas tampak tidak ada null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19587b8d",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13438336",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_labels = ['Male', 'Female']\n",
    "c_labels = ['Non Churn(0)', 'Candidat Chrun(1)']\n",
    "# Create subplots: use 'domain' type for Pie subplot\n",
    "fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\n",
    "fig.add_trace(go.Pie(labels=g_labels, values=df['Gender'].value_counts(), name=\"Gender\"),\n",
    "              1, 1)\n",
    "fig.add_trace(go.Pie(labels=c_labels, values=df['Exited'].value_counts(), name=\"Exited\"),\n",
    "              1, 2)\n",
    "\n",
    "# Use `hole` to create a donut-like pie chart\n",
    "fig.update_traces(hole=.3, hoverinfo=\"label+percent+name\", textfont_size=16)\n",
    "fig.update_layout(\n",
    "    title_text=\"Gender and Exited Distributions\",\n",
    "    # Add annotations in the center of the donut pies.\n",
    "    annotations=[dict(text='Gender', x=0.18, y=0.5, font_size=18, showarrow=False),\n",
    "                 dict(text='Exited', x=0.81, y=0.5, font_size=20, showarrow=False)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b7fd3",
   "metadata": {},
   "source": [
    "+ 20,4% nasabah berpindah ke perusahaan bank yang lain\n",
    "+ Laki laki lebih menominasi dari sisi kepelangganan layanan telekomunikasi ini sebesar 54,6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2536b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita akan hitung secara angka berapa banyak pelanggan yang masih bertahan ( No Churn) \n",
    "# di urutkan berdasarkan jenis kelamin\n",
    "\n",
    "df['Exited'][df['Exited']==0].groupby(by=df['Gender']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita juga akan hitung secara angka berapa banyak pelanggan yang churn \n",
    "# di urutkan berdasarkan jenis kelamin\n",
    "\n",
    "df['Exited'][df['Exited']==1].groupby(by=df['Gender']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30580751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita visualize kan gabungan data pelanggan tersebut\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "labels =[\"Exited: Candidat Chrun\",\"Exited:Non Churn\"]\n",
    "values = [2037,7963]\n",
    "labels_gender = [\"F\",\"M\",\"F\",\"M\"]\n",
    "sizes_gender = [1139,898 , 3404,4559]\n",
    "colors = ['#ff6666', '#66b3ff']\n",
    "colors_gender = ['#c2c2f0','#ffb3e6', '#c2c2f0','#ffb3e6']\n",
    "explode = (0.3,0.3) \n",
    "explode_gender = (0.1,0.1,0.1,0.1)\n",
    "textprops = {\"fontsize\":15}\n",
    "#Plot\n",
    "plt.pie(values, labels=labels,autopct='%1.1f%%',pctdistance=1.08, labeldistance=0.8,colors=colors, startangle=90,frame=True, explode=explode,radius=10, textprops =textprops, counterclock = True, )\n",
    "plt.pie(sizes_gender,labels=labels_gender,colors=colors_gender,startangle=90, explode=explode_gender,radius=7, textprops =textprops, counterclock = True, )\n",
    "#Draw circle\n",
    "centre_circle = plt.Circle((0,0),5,color='black', fc='white',linewidth=0)\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.title('Churn Distribution w.r.t Gender: Male(M), Female(F)', fontsize=24, y=1.1)\n",
    "\n",
    "# show plot \n",
    " \n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1624c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita lihat distribusi data customer jika ditinjauan Jumlah produk layanan perbankan yang di ambil \n",
    "\n",
    "fig = px.histogram(df, x=\"Exited\", color=\"NumOfProducts\", barmode=\"group\", title=\"<b>Customer Number Of Product distribution<b>\")\n",
    "fig.update_layout(width=600, height=500, bargap=0.1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194fcc58",
   "metadata": {},
   "source": [
    "# 3. EDA (Exploratory of Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b8a51",
   "metadata": {},
   "source": [
    "## 3.1 Data Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627825dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable_name = \"Exited\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare():\n",
    "    df_prep = df.copy()\n",
    "    \n",
    "    \n",
    "    missing_value_len = df.isnull().any().sum()\n",
    "    if missing_value_len == 0:\n",
    "        print(\"No Missing Value\")\n",
    "    else:\n",
    "        print(\"Investigate Missing Value, Missing Value : \" + str(missing_value_len))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    show_unique_count_variables(df = df_prep)\n",
    "    \n",
    "    \n",
    "    df_prep['Tenure'] =  df_prep.Tenure.astype(np.float)\n",
    "    df_prep['NumOfProducts'] =  df_prep.NumOfProducts.astype(np.float)\n",
    "    return df_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d702a",
   "metadata": {},
   "source": [
    "### Representasi data nilai unik pada semua variabel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_unique_count_variables(df):\n",
    "    for index, value in df.nunique().items():\n",
    "        print(str(index) + \"\\n\\t\\t\\t:\" + str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0ebe4",
   "metadata": {},
   "source": [
    "## 3.2. Outliers Observe (LOF method and Supress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31415660",
   "metadata": {},
   "source": [
    "#### 3.2.1. Outlier Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efca9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_outliers(df):\n",
    "    \n",
    "# mengamati fitur outlier dengan melihat semua kolom dari atas\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    df_num_cols = df.select_dtypes(include=numerics)\n",
    "    sns.set(font_scale = 0.7) \n",
    "    fig, axes = plt.subplots(nrows = 2, ncols = 5, gridspec_kw =  dict(hspace=0.3), figsize = (12,9))\n",
    "    fig.tight_layout()\n",
    "    for ax,col in zip(axes.flatten(), df_num_cols.columns):\n",
    "        sns.boxplot(x = df_num_cols[col], color='green', ax = ax)\n",
    "    fig.suptitle('Observing Outliers', color = 'r', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecaf3f",
   "metadata": {},
   "source": [
    "#### 3.2.2. Visualization of outliers according to the LOF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lof_observation(df):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    df_num_cols = df.select_dtypes(include=numerics)\n",
    "    df_outlier = df_num_cols.astype(\"float64\")\n",
    "    clf = LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\n",
    "    clf.fit_predict(df_outlier)\n",
    "    df_scores = clf.negative_outlier_factor_\n",
    "    scores_df = pd.DataFrame(np.sort(df_scores))\n",
    "    \n",
    "    scores_df.plot(stacked=True, xlim = [0,20], color='r', title='Visualization of outliers according to the LOF method', style = '.-');                # first 20 observe\n",
    "    th_val = np.sort(df_scores)[2]\n",
    "    outliers = df_scores > th_val\n",
    "    df = df.drop(df_outlier[~outliers].index)\n",
    "    df.shape\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956270a2",
   "metadata": {},
   "source": [
    "#### 3.2.3 Outiler suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_outliers(df):\n",
    "    \n",
    "    # Penekanan credit Score\n",
    "    \n",
    "    Q1 = df[\"Age\"].quantile(0.25)\n",
    "    Q3 = df[\"Age\"].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    print(\"When age and credit score is printed below lower score: \", lower, \"and upper score: \", upper)\n",
    "    df_outlier = df[\"Age\"][(df[\"Age\"] > upper)]\n",
    "    df[\"Age\"][df_outlier.index] = upper\n",
    "    \n",
    "    \n",
    "    # Kredit Score\n",
    "    \n",
    "    Q1 = df[\"CreditScore\"].quantile(0.25)\n",
    "    Q3 = df[\"CreditScore\"].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    print(\"When age and credit score is printed above lower score: \", lower, \"and upper score: \", upper)\n",
    "    df_outlier = df[\"CreditScore\"][(df[\"CreditScore\"] < lower)]\n",
    "    df[\"CreditScore\"][df_outlier.index] = lower\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b64586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_process(df):\n",
    "    #show_outliers(df = df)\n",
    "    df_outlier = lof_observation(df = df)\n",
    "    df_outlier = clear_outliers(df = df_outlier)\n",
    "    return df_outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7ec80",
   "metadata": {},
   "source": [
    "## 3.3. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70612f93",
   "metadata": {},
   "source": [
    "### 3.3.1. Dependent variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49660212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dependent_variable(df):\n",
    "#     sns.countplot(data = df, x = dependent_variable_name, label = 'Count') \\\n",
    "#     .set_title(dependent_variable_name + ' dependent variable situation', fontsize = 18, color = 'r')\n",
    "    fig, axarr = plt.subplots(2, 3, figsize=(18, 6))\n",
    "    sns.countplot(x = 'Geography', hue = 'Exited',data = df, ax = axarr[0][0])\n",
    "    sns.countplot(x = 'Gender', hue = 'Exited',data = df, ax = axarr[0][1])\n",
    "    sns.countplot(x = 'HasCrCard', hue = 'Exited',data = df, ax = axarr[0][2])\n",
    "    sns.countplot(x = 'IsActiveMember', hue = 'Exited',data = df, ax = axarr[1][0])\n",
    "    sns.countplot(x = 'NumOfProducts', hue = 'Exited',data = df, ax = axarr[1][1])\n",
    "    sns.countplot(x = 'Tenure', hue = 'Exited',data = df, ax = axarr[1][2])\n",
    "    zero, one = df[dependent_variable_name].value_counts()\n",
    "    print(\"Dependent variable distribution;\")\n",
    "    print(dependent_variable_name + \" 0 count:\", zero)\n",
    "    print(dependent_variable_name + \" 1 count:\", one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f5558",
   "metadata": {},
   "source": [
    "### 3.3.2. Numeric columns distribution observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57dd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_numeric_columns_distributions(df):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    df_num_cols = df.select_dtypes(include=numerics)\n",
    "    columns = df_num_cols.columns[: len(df_num_cols.columns)]\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(18, 15)\n",
    "    #plt.subplots(figsize=(22,22))\n",
    "    length = len(columns)\n",
    "    for i,j in itertools.zip_longest(columns, range(length)):\n",
    "        plt.subplot((length / 2), 3, j+1)\n",
    "        plt.subplots_adjust(wspace = 0.2, hspace = 0.5)\n",
    "        df_num_cols[i].hist(bins = 20, edgecolor = 'black')\n",
    "        plt.title(i)\n",
    "    fig = fig.suptitle('Structures of numeric variables', color = 'r' ,fontsize = 18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b1e61",
   "metadata": {},
   "source": [
    "### 3.3.3. Status of other variables according to dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301983cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dependent_variable_cross_others_distributions(df):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    df_dependent_var = df[df[dependent_variable_name] == 1]\n",
    "    df_num_cols = df_dependent_var.select_dtypes(include = numerics)\n",
    "    columns = df_num_cols.columns[: len(df_num_cols.columns)]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(18, 15)\n",
    "    length = len(columns)\n",
    "    for i,j in itertools.zip_longest(columns, range(length)):\n",
    "        plt.subplot((length / 2), 3, j+1)\n",
    "        plt.subplots_adjust(wspace = 0.2, hspace = 0.5)\n",
    "        df_num_cols[i].hist(bins = 20, edgecolor = 'black')\n",
    "        plt.title(i)\n",
    "    fig = fig.suptitle(dependent_variable_name + ' Status of other variables according to 1 dependent variable', color = 'r', fontsize = 18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5182b",
   "metadata": {},
   "source": [
    "### 3.3.4. Categorical variables are observed according to the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4823b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dependent_variable_cross_categorical_distributions(df, categorical_columns):\n",
    "    sns.set(font_scale = 0.7) \n",
    "    fig, axes = plt.subplots(nrows = int( len(categorical_columns) / 2 ) , ncols = 2, figsize = (7,9))\n",
    "    fig.tight_layout()\n",
    "    for ax,col in zip(axes.flatten(), categorical_columns):\n",
    "        sns.countplot(x = df[col], hue = dependent_variable_name, data = df, ax = ax)\n",
    "    fig.suptitle('Categorical variables are monitored according to the dependent variable', color = 'r', fontsize = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f4b371",
   "metadata": {},
   "source": [
    "### 3.3.5. The main method that started all data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_analysis(df):\n",
    "    show_dependent_variable(df)\n",
    "    show_numeric_columns_distributions(df)\n",
    "    show_dependent_variable_cross_others_distributions(df)\n",
    "    show_dependent_variable_cross_categorical_distributions(df = df_outlier, categorical_columns = [\"Gender\",\"Geography\",\"HasCrCard\",\"IsActiveMember\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37138bd",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008596a",
   "metadata": {},
   "source": [
    "### 4.1. Credit Score grouping ( min= 358 and max= 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_score_table(row):\n",
    "    \n",
    "    credit_score = row.CreditScore\n",
    "    if credit_score >= 300 and credit_score < 500:\n",
    "        return \"Very_Poor\"\n",
    "    elif credit_score >= 500 and credit_score < 601:\n",
    "        return \"Poor\"\n",
    "    elif credit_score >= 601 and credit_score < 661:\n",
    "        return \"Fair\"\n",
    "    elif credit_score >= 661 and credit_score < 781:\n",
    "        return \"Good\"\n",
    "    elif credit_score >= 851:\n",
    "        return \"Top\"\n",
    "    elif credit_score >= 781 and credit_score < 851:\n",
    "        return \"Excellent\"\n",
    "    elif credit_score < 300:\n",
    "        return \"Deep\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13583b25",
   "metadata": {},
   "source": [
    "### 4.2. Product utilization RATE by YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba38a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_utilization_rate_by_year(row):\n",
    "    number_of_products = row.NumOfProducts\n",
    "    tenure = row.Tenure\n",
    "    \n",
    "    if number_of_products == 0:\n",
    "        return 0\n",
    "    \n",
    "    if tenure == 0:\n",
    "        return number_of_products\n",
    "    \n",
    "    rate = number_of_products / tenure\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c8b529",
   "metadata": {},
   "source": [
    "### 4.3. Product utilization rate by estimated SALARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_utilization_rate_by_estimated_salary(row):\n",
    "    number_of_products = row.number_of_products\n",
    "    estimated_salary = row.EstimatedSalary\n",
    "    \n",
    "    if number_of_products == 0:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    rate = number_of_products / estimated_salary\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd43e81",
   "metadata": {},
   "source": [
    "### 4.4. According to countries monthly average salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fab19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countries_monthly_average_salaries(row):\n",
    "    #brutto datas from  https://tr.wikipedia.org/wiki/Aylık_ortalama_ücretlerine_göre_Avrupa_ülkeleri_listesi\n",
    "    fr = 3696    \n",
    "    de = 4740\n",
    "    sp = 2257\n",
    "    salary = row.EstimatedSalary / 12\n",
    "    country = row.Geography              # Germany, France and Spain\n",
    "    \n",
    "    if country == 'Germany':\n",
    "        return salary / de\n",
    "    elif country == \"France\":\n",
    "        return salary / fr\n",
    "    elif country == \"Spain\": \n",
    "        return salary / sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4086c4b",
   "metadata": {},
   "source": [
    "### 4.5. The main method that started all Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, is_show_graph = False):\n",
    "    df_fe = df.copy()\n",
    "    \n",
    "    #bakiye_maas_orani\n",
    "    balance_salary_rate = 'balance_salary_rate'\n",
    "    df_fe[balance_salary_rate] = df_fe.Balance / df_fe.EstimatedSalary\n",
    "    \n",
    "    #yila_gore_urun_kullanim_orani\n",
    "    df_fe = df_fe.assign(product_utilization_rate_by_year=df_fe.apply(lambda x: product_utilization_rate_by_year(x), axis=1)) \n",
    "    \n",
    "    #tahmini_maasa_gore_urun_kullanim_orani\n",
    "    #df_fe = df_fe.assign(product_utilization_rate_by_estimated_salary = df_fe.apply(lambda x: product_utilization_rate_by_estimated_salary(x), axis=1)) \n",
    "    \n",
    "    \n",
    "    #musteri_yilina göre yaşa göre standardize edilmesi, oranlanmasi - ergenlik donemini cikariyoruz!\n",
    "    tenure_rate_by_age = 'tenure_rate_by_age'\n",
    "    df_fe[tenure_rate_by_age] = df_fe.Tenure / (df_fe.Age-17)\n",
    "    \n",
    "    #yaşa göre kredi_skoru standardize edilmesi, oranlanmasi - ergenlik donemini cikariyoruz!\n",
    "    credit_score_rate_by_age = 'credit_score_rate_by_age'\n",
    "    df_fe[credit_score_rate_by_age] = df_fe.CreditScore / (df_fe.Age-17)\n",
    "    \n",
    "    #maaşa gore kullanilan urun oran, oranlanmasi\n",
    "    product_utilization_rate_by_salary = 'product_utilization_rate_by_salary'\n",
    "    #df_fe[product_utilization_rate_by_salary] = df_fe.Tenure / (df_fe.EstimatedSalary)\n",
    "   \n",
    "    #maaşa göre kredi_skoru urun oran, oranlanmasi\n",
    "    credit_score_rate_by_salary = 'credit_score_rate_by_salary'\n",
    "    df_fe[credit_score_rate_by_salary] = df_fe.CreditScore / (df_fe.EstimatedSalary)\n",
    "    \n",
    "    #Feature Eng. oluşturulan değişkenlerin bağımlı değişkene göre grafikleri gösterilsin mi?\n",
    "    if is_show_graph:\n",
    "        fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (20,12))\n",
    "        fig.tight_layout()\n",
    "        sns.boxplot(y = balance_salary_rate, x = dependent_variable_name, hue = dependent_variable_name, data = df_fe, ax = axes[0][0])\n",
    "        sns.boxplot(y = product_utilization_rate_by_year, x = dependent_variable_name, hue = dependent_variable_name, data = df_fe, ax = axes[0][1])\n",
    "        #sns.countplot(x = credit_score_rate_by_age, hue = dependent_variable_name, data = df_fe, ax = axes[1][0])\n",
    "        #sns.countplot(x = credit_score_rate_by_age, hue = dependent_variable_name, data = df_fe, ax = axes[1][1])\n",
    "        plt.ylim(-1, 5)\n",
    "    \n",
    "    \n",
    "    #feature engineering add- kredi_skor_tablosu\n",
    "    df_fe = df_fe.assign(credit_score_table=df_fe.apply(lambda x: credit_score_table(x), axis=1))\n",
    "    \n",
    "    #feature engineering add- ulkelere ortalama  maas durum\n",
    "    df_fe = df_fe.assign(countries_monthly_average_salaries = df_fe.apply(lambda x: countries_monthly_average_salaries(x), axis=1)) \n",
    "    \n",
    "    return df_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe6c76",
   "metadata": {},
   "source": [
    "## 5. Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad197f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vucut_yag_orani, vucut_yag_orani_kategori,beden_kitle_kategori,insulin_kategori,kucuk_tansiyon_kategori\n",
    "\n",
    "def data_encoding(df):\n",
    "    df_model = df.copy()\n",
    "    '''\n",
    "    # It was attempted to reduce the number of 0 observations.\n",
    "    churn_zero = df_model[df_model.apply(lambda x: True if x['Exited'] == 0 else False , axis=1)]\n",
    "    df_train = churn_zero.sample(frac=0.1,random_state=100)\n",
    "    df_model = df_model.drop(df_train.index)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # >>>> Categorical columns <<<<<\n",
    "    \n",
    "    non_encoding_columns = [\"Geography\",\"HasCrCard\",\"IsActiveMember\",\"Gender\",\"NumOfProducts\",\"Tenure\",\"credit_score_table\"]\n",
    "    \n",
    "    df_non_encoding = df_model[non_encoding_columns]\n",
    "    df_model = df_model.drop(non_encoding_columns,axis=1)\n",
    "    \n",
    "    \n",
    "    df_encoding = df_non_encoding.copy()\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    df_encoding[\"gender_category\"] = encoder.fit_transform(df_non_encoding.Gender)\n",
    "    df_encoding[\"country_category\"] = encoder.fit_transform(df_non_encoding.Geography)\n",
    "    df_encoding[\"credit_score_category\"] = encoder.fit_transform(df_non_encoding.credit_score_table)\n",
    "\n",
    "    \n",
    "\n",
    "    df_encoding.reset_index(drop=True, inplace=True)\n",
    "    df_model.reset_index(drop=True, inplace=True)\n",
    "    df_model = pd.concat([df_model,df_encoding],axis=1)\n",
    "\n",
    "    df_model = df_model.drop([\"Geography\",\"Gender\",\"CustomerId\",\"Surname\",\"credit_score_table\",\"CreditScore\",\"EstimatedSalary\"],axis=1)\n",
    "    df_model = df_model.reset_index()\n",
    "    df_model = df_model.drop('index',axis=1)\n",
    "    \n",
    "    df_model.loc[df_model.HasCrCard == 0, 'credit_card_situation'] = -1\n",
    "    df_model.loc[df_model.IsActiveMember == 0, 'is_active_member'] = -1\n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff49c6e",
   "metadata": {},
   "source": [
    "## 6. Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5398c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prepare(df_model):\n",
    "    y = df_model[dependent_variable_name]\n",
    "    X = df_model.loc[:, df_model.columns != dependent_variable_name]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 12345)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform (X_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80885708",
   "metadata": {},
   "source": [
    "### 6.1 Part of Data TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f46176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_training(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    models = []\n",
    "    models.append(('LOGR',LogisticRegression()))\n",
    "    models.append(('KNN',KNeighborsClassifier()))\n",
    "    models.append(('CART',DecisionTreeClassifier()))\n",
    "    models.append(('RF',RandomForestClassifier()))\n",
    "    #models.append(('SVC',SVC()))\n",
    "    models.append(('GBM',GradientBoostingClassifier()))\n",
    "    models.append(('XGBoost',XGBClassifier()))\n",
    "    models.append(('LightGBM',LGBMClassifier()))\n",
    "    models.append(('CatBoost',CatBoostClassifier()))\n",
    "\n",
    "    df_result = pd.DataFrame(columns=[\"model\",\"accuracy_score\",\"scale_method\",\"0_precision\",\"0_recall\",\"1_precision\",\"1_recall\"])\n",
    "    index = 0\n",
    "    for name,model in models:\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = accuracy_score(y_test,y_pred)\n",
    "        class_report = classification_report(y_test,y_pred,digits=2,output_dict=True)\n",
    "        zero_report = class_report['0']\n",
    "        one_report = class_report['1']\n",
    "        df_result.at[index,['model','accuracy_score','scale_method',\"0_precision\",\"0_recall\",\"1_precision\",\"1_recall\"]] = [name,score,\"NA\",zero_report['precision'],zero_report['recall'],one_report['precision'],one_report['recall']]\n",
    "        index += 1\n",
    "    return df_result.sort_values(\"accuracy_score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08242b",
   "metadata": {},
   "source": [
    "## 7. HELPing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to give best model score and parameters\n",
    "\n",
    "def best_model(model):\n",
    "    print(model.best_score_)    \n",
    "    print(model.best_params_)\n",
    "    print(model.best_estimator_)\n",
    "    \n",
    "def get_auc_scores(y_actual, method,method2):\n",
    "    auc_score = roc_auc_score(y_actual, method); \n",
    "    fpr_df, tpr_df, _ = roc_curve(y_actual, method2); \n",
    "    return (auc_score, fpr_df, tpr_df)\n",
    "\n",
    "\n",
    "from matplotlib import rc,rcParams\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, color=\"blue\")\n",
    "    plt.yticks(tick_marks, classes, color=\"blue\")\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"red\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "''' inf yakalama\n",
    "indices_to_keep = df_encoded.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "df_encoded[indices_to_keep].astype(np.float64)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c1522",
   "metadata": {},
   "source": [
    "## 8. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33212ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = data_prepare()\n",
    "\n",
    "df_outlier = outlier_process(df = df_prep)\n",
    "\n",
    "\n",
    "show_data_analysis(df_prep)\n",
    "show_outliers(df = df_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ff8c6",
   "metadata": {},
   "source": [
    "+ Let's see after feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = feature_engineering(df = df_outlier)\n",
    "df_fe.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71286f1f",
   "metadata": {},
   "source": [
    "+ And see after data encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = data_encoding(df_fe)\n",
    "df_encoded.drop(['credit_card_situation', 'is_active_member'], axis=1, inplace=True)\n",
    "df_encoded.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca65608",
   "metadata": {},
   "source": [
    "## Now, let's see correlation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f088d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df_encoded.corr().abs()\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(correlation, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8269e4",
   "metadata": {},
   "source": [
    "## List of correlation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_results = df_encoded.corrwith(df_encoded[\"Exited\"]).abs().nlargest(24)\n",
    "corrs_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cd8bd6",
   "metadata": {},
   "source": [
    "# 9. Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_prepare test, train split 0.2\n",
    "X_train, X_test, y_train, y_test = model_prepare(df_model = df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19de08e",
   "metadata": {},
   "source": [
    "### 9.1. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2ebae",
   "metadata": {},
   "source": [
    "+ Have a look with Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ab584",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_model = LogisticRegression().fit(X_train,y_train)\n",
    "y_pred = logr_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Accuracy score of Logistic Regression: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08bb2a",
   "metadata": {},
   "source": [
    "### 9.2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b34a8",
   "metadata": {},
   "source": [
    "+ This is for all LogisticRegression, RandomForestClassifier, GradientBoostingClassifier, XGBClassifier, and LGBMClassifier model objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd262b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_result = data_training(X_train, X_test, y_train, y_test)\n",
    "training_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e5028",
   "metadata": {},
   "source": [
    "+ Now, let's have a look which model is best for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4aecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b8580",
   "metadata": {},
   "source": [
    "# 10. Model Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a86f6",
   "metadata": {},
   "source": [
    "## 10.1. XGBoost Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model=XGBClassifier(silent=0, learning_rate=0.23, max_delta_step=5,\n",
    "                            objective='reg:logistic',n_estimators=92, \n",
    "                            max_depth=5, eval_metric=\"logloss\", gamma=3,base_score=0.5)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred,digits=2))\n",
    "print(\"Accuracy score of Tuned XGBoost Regression: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e65504",
   "metadata": {},
   "source": [
    "## 10.2. Random Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283404b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [3, 5, 6, 7, 8], 'max_features': [2,4,6,7,8,9],'n_estimators' : [50,100], 'min_samples_split': [3, 5, 6, 7]}\n",
    "randFor_grid = GridSearchCV(RandomForestClassifier(), param_grid, cv = 5, refit = True, verbose = 0)\n",
    "randFor_grid.fit(X_train,y_train)\n",
    "best_model(randFor_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e58ad",
   "metadata": {},
   "source": [
    "+ Using the parameters and get final version accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_model = RandomForestClassifier(max_depth=8, max_features=6, min_samples_split=6,n_estimators=50)\n",
    "rnd_model.fit(X_train, y_train)\n",
    "y_pred = rnd_model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred,digits=2))\n",
    "print(\"Accuracy score of tuned Random Forest model: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862f937",
   "metadata": {},
   "source": [
    "## 10.3. LightGBM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d11e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = LGBMClassifier(silent = 0, learning_rate = 0.09, max_delta_step = 2, n_estimators = 100, boosting_type = 'gbdt',\n",
    "                            max_depth = 10, eval_metric = \"logloss\", gamma = 3, base_score = 0.5)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=2))\n",
    "print(\"Accuracy score of tuned LightGBM model: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbfa621",
   "metadata": {},
   "source": [
    "# 11. Confussion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72fe4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_test, y_pred=y_pred)\n",
    "plot_confusion_matrix(cfm, classes=['Non Churn','Churn'],\n",
    "                      title='Churn Confusion matrix')\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = cfm.ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c1c756",
   "metadata": {},
   "source": [
    "# 12. ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ca1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "y_pred_proba = lgbm_model.predict_proba(X_test)\n",
    "skplt.metrics.plot_roc_curve(y_test, y_pred_proba, figsize=(8,8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4100b0",
   "metadata": {},
   "source": [
    "# 13. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = df_encoded.loc[:, df_encoded.columns != dependent_variable_name]\n",
    "\n",
    "feature_importance = pd.Series(lgbm_model.feature_importances_, \n",
    "                               index=feature_index.columns).sort_values(ascending=False)\n",
    "sns.barplot(x = feature_importance, y = feature_importance.index, color='r', saturation=1)\n",
    "plt.xlabel('Variable Severity Scores')\n",
    "plt.ylabel('Variables')\n",
    "plt.title('Variable Severity Levels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5dca7",
   "metadata": {},
   "source": [
    "# 14. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39ab72d",
   "metadata": {},
   "source": [
    "+ Tujuan kami dalam proyek ini adalah mengembangkan model prediksi churn menggunakan algoritme pembelajaran mesin. \n",
    "+ Ada 10.000 baris dalam kumpulan data dan tidak ada nilai yang hilang. dan dataset terdiri dari 13 variabel. \n",
    "+ Kesimpulan berikut didapat dari analisis fitur: \n",
    "    + Sebagian besar nasabah yang menggunakan produk 3 dan 4 berhenti bekerja sama dengan bank. \n",
    "    + Bahkan, semua pelanggan yang menggunakan produk nomor 4 sudah tidak ada. \n",
    "    + Pelanggan berusia antara 40 dan 65 tahun lebih cenderung keluar dari bank. \n",
    "    + Mereka yang memiliki skor kredit di bawah 450 memiliki tingkat pengabaian yang tinggi. \n",
    "    + Prediksi dilakukan dengan total 8 model klasifikasi. \n",
    "    + Head tertinggi diambil dengan metode LightGBM. \n",
    "    + Skor akurasi dan validasi silang dihitung untuk setiap model dan hasilnya ditampilkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2806d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
